{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_ResNet+GeM.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7cVif9HYqiSs","colab_type":"text"},"source":["**PART 2:  Basic Resnet+GeM**\n"]},{"cell_type":"code","metadata":{"id":"WlVwmKYFDPcy","colab_type":"code","outputId":"f490ee85-2f71-4e07-aaf1-8f6591a01b5f","executionInfo":{"status":"ok","timestamp":1559792177258,"user_tz":420,"elapsed":497,"user":{"displayName":"Ekaterina Kastrama","photoUrl":"","userId":"16919877199024167270"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_L9PqHKcDybt","colab_type":"code","outputId":"9352511a-ecd6-4a22-b38e-5fb2451fe0e5","executionInfo":{"status":"ok","timestamp":1559792180438,"user_tz":420,"elapsed":1929,"user":{"displayName":"Ekaterina Kastrama","photoUrl":"","userId":"16919877199024167270"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pylab as plt\n","import os\n","import gc\n","import seaborn as sns\n","import tensorflow as tf\n","import scipy.misc\n","\n","from os import listdir\n","from os.path import isfile, join\n","\n","from skimage.transform import resize\n","\n","import warnings\n","print('TF',tf.__version__)\n","print('NP',np.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TF 1.13.1\n","NP 1.16.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jjGNQQCGUbDr","colab_type":"code","outputId":"32aef17f-fa04-4740-eb95-7a86838fab0a","executionInfo":{"status":"ok","timestamp":1559792208631,"user_tz":420,"elapsed":13121,"user":{"displayName":"Ekaterina Kastrama","photoUrl":"","userId":"16919877199024167270"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def download_labels():\n","  \n","  if  os.path.isfile('train.csv')==False:\n","\n","    !wget 'https://s3.amazonaws.com/google-landmark/metadata/train.csv'\n","\n","  train_data = pd.read_csv('train.csv')\n","  print('Number of examples: ',len(train_data))\n","  \n","  return train_data\n","  \n","train_data=download_labels()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of examples:  4132914\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ygI4Ed8hU65C","colab_type":"code","outputId":"712cf087-a32a-4c93-cb80-c237e1b472ac","executionInfo":{"status":"ok","timestamp":1559792208635,"user_tz":420,"elapsed":11089,"user":{"displayName":"Ekaterina Kastrama","photoUrl":"","userId":"16919877199024167270"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(\"Training data size\",train_data.shape)\n","#print(\"test data size\",test_data.shape)\n","#submission.head()\n","#Training data size (1 225 029, 3)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training data size (4132914, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7jOi8aybrjSA","colab_type":"code","outputId":"09e213c9-643d-4f0c-f8f9-07cddda4fa0b","executionInfo":{"status":"ok","timestamp":1559792212694,"user_tz":420,"elapsed":1086,"user":{"displayName":"Ekaterina Kastrama","photoUrl":"","userId":"16919877199024167270"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import requests\n","import random\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from tqdm import tqdm\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import threading\n","import urllib\n","import cv2\n","import time\n","\n","import keras\n","from keras.applications import ResNet50,InceptionResNetV2\n","from keras.applications import DenseNet121\n","from keras import backend as K\n","from keras import regularizers\n","from keras.engine.topology import Input\n","from keras.layers import Activation, Add, BatchNormalization, Concatenate, Conv2D, Dense, Flatten, GlobalMaxPooling2D, \\\n","    Lambda, MaxPooling2D, Reshape, Dropout\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras.preprocessing.image import img_to_array\n","from keras.utils import Sequence\n","from sklearn.model_selection import train_test_split\n","\n","from collections import Counter\n","\n","import os"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"P0Qg4cLVrWBx","colab_type":"code","outputId":"3eab5a22-d805-4a96-d8bc-4c701fa7a644","executionInfo":{"status":"ok","timestamp":1559792288279,"user_tz":420,"elapsed":5435,"user":{"displayName":"Ekaterina Kastrama","photoUrl":"","userId":"16919877199024167270"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["\n","NUM_THRESHOLD = 250\n","train=train_data\n","\n","counts = dict(Counter(train['landmark_id']))\n","landmarks_dict = {x:[] for x in train.landmark_id.unique() if counts[x] >= NUM_THRESHOLD}\n","NUM_CLASSES = len(landmarks_dict)\n","print(\"Total number of valid classes: {}\".format(NUM_CLASSES))\n","\n","i = 0\n","landmark_to_idx = {}\n","idx_to_landmark = []\n","for k in landmarks_dict:\n","    landmark_to_idx[k] = i\n","    idx_to_landmark.append(k)\n","    i += 1\n","\n","all_urls = train['url'].tolist()\n","all_landmarks = train['landmark_id'].tolist()\n","valid_urls_dict = {x[0].split(\"/\")[-1]:landmark_to_idx[x[1]] for x in zip(all_urls, all_landmarks) if x[1] in landmarks_dict}\n","valid_urls_list = [x[0] for x in zip(all_urls, all_landmarks) if x[1] in landmarks_dict]\n","\n","NUM_EXAMPLES = len(valid_urls_list)\n","print(\"Total number of valid examples: {}\".format(NUM_EXAMPLES))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Total number of valid classes: 1067\n","Total number of valid examples: 478577\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FxH3BEB8qxdH","colab_type":"code","colab":{}},"source":["train1_urls, test_urls = train_test_split(valid_urls_list, test_size=3*NUM_CLASSES/NUM_EXAMPLES,random_state=1234)\n","train_urls, validation_urls = train_test_split(train1_urls, test_size=3*NUM_CLASSES/NUM_EXAMPLES,random_state=1234)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3FvRHtb59MI","colab_type":"code","colab":{}},"source":["def download_image_cv2_urllib(url):\n","    \"\"\"\n","    Modifying the url to download the 360p or 720p version actually slows it down. \n","    \"\"\"\n","    try:\n","        resp = urllib.request.urlopen(url)\n","        foo = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n","        img_original = cv2.imdecode(foo, cv2.IMREAD_COLOR)\n","        foo = cv2.resize(img_original,(128, 128), interpolation=cv2.INTER_AREA)\n","        return foo,img_original\n","    except:\n","        return np.array([]),np.array([])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"smhB6w0isdI8","colab_type":"code","colab":{}},"source":["## downloading validation set\n","\n","def download_urls(urls):\n","\n","  images = []\n","  y_label = []\n","  \n","  i=0\n","  for url in urls:\n","    im,_ = download_image_cv2_urllib(url)\n","    if im.size != 0:\n","        i=+i\n","        images.append(im)\n","        y_label.append(valid_urls_dict[url.split(\"/\")[-1]])\n","    if i%500==0 and i>500:\n","      print('Downloaded: '+str(i))\n","    \n","  x = np.array(images)\n","  y = np.zeros((len(images), NUM_CLASSES))\n","        \n","  for i in range(len(y_label)):\n","    y[i,y_label[i]] = 1.\n","    \n","  return x,y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L304eQtw1ykn","colab_type":"code","colab":{}},"source":["valid_x,valid_y=download_urls(validation_urls)\n","test_x,test_y=download_urls(test_urls)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9IrRKYx0iC6K","colab_type":"code","colab":{}},"source":["#print(len(test_x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHIwhhOZtsCJ","colab_type":"code","colab":{}},"source":["def randomCrop(img, width, height):\n","\n","    \n","    x = random.randint(0, max(img.shape[1] - width,0))\n","    y = random.randint(0, max(img.shape[0] - height,0))\n","    \n","    img_crop = img[y:y+height, x:x+width]\n","\n","    return img_crop\n","\n","\n","\n","class DataGen(Sequence):\n","    def __init__(self, data, batch_size=24, verbose=1):\n","        self.batch_size=batch_size\n","        self.data_urls = data\n","\n","    def normalize(self,data):\n","        return data\n","    \n","    def __getitem__(self, index):\n","      \n","       \n","        number_of_crops=1\n","        batch_urls = random.sample(self.data_urls, int(self.batch_size/number_of_crops))\n","        \n","        output = []\n","        y_classes = []\n","        \n","        for url in batch_urls:\n","            im,original = download_image_cv2_urllib(url)\n","            if im.size != 0:\n","              \n","                output.append(im)\n","                y_classes.append(valid_urls_dict[url.split(\"/\")[-1]])\n","                if original.shape[0]<=128 or original.shape[1]<=128: \n","                  original=cv2.resize(original,(256, 256), interpolation=cv2.INTER_AREA)\n","                for k in range(number_of_crops-1):\n","             \n","                  im=randomCrop(original,128,128)\n","                  output.append(im)\n","                  y_classes.append(valid_urls_dict[url.split(\"/\")[-1]])\n","                  \n","        \n","        x = np.array(output)\n","  \n","        y = np.zeros((len(output), NUM_CLASSES))\n","        \n","        for i in range(len(y_classes)):\n","            y[i,y_classes[i]] = 1.\n","        \n","  \n","        return x,y\n","            \n","    def on_epoch_end(self):\n","        return\n","\n","    def __len__(self):\n","        #return len(valid_urls_list) // self.batch_size\n","        return 50\n","      \n","def accuracy_class(y_true, y_pred):\n","    true = K.argmax(y_true, axis=1)\n","    pred = K.argmax(y_pred, axis=1)\n","    matches = K.equal(true, pred)\n","    return K.mean(matches)\n","\n","#adapted from https://github.com/jandaldrop/landmark-recognition-challenge/blob/master/landmarks-xception.ipynb\n","\n","def batch_GAP(y_t, y_p):\n","  \n","    import tensorflow as tf\n","    pred_cat = tf.argmax(y_p, axis=-1)    \n","    y_t_cat = tf.argmax(y_t, axis=-1) * tf.cast(\n","        tf.reduce_sum(y_t, axis=-1), tf.int64)\n","    \n","    n_pred = tf.shape(pred_cat)[0]\n","    is_c = tf.cast(tf.equal(pred_cat, y_t_cat), tf.float32)\n","\n","    GAP = tf.reduce_mean(\n","          tf.cumsum(is_c) * is_c / tf.cast(\n","              tf.range(1, n_pred + 1), \n","              dtype=tf.float32))\n","    \n","    return GAP  \n","  \n","def binary_crossentropy_n_cat(y_t, y_p):\n","    return keras.metrics.binary_crossentropy(y_t, y_p) * NUM_CLASSES\n","\n","#GeM\n","\n","gm_exp = tf.Variable(3., dtype=tf.float32)\n","def generalized_mean_pool_2d(X):\n","    \n","\n","    import tensorflow as tf\n","\n","    \n","    pool = (tf.reduce_mean(tf.abs(X**(gm_exp)), \n","                           axis=[1,2], \n","                           keepdims=False)+1.e-8)**(1./gm_exp)\n","    return pool"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XLZWcG0Qidg-","colab_type":"code","outputId":"9cd8b23d-5729-45e7-de90-bc495300ced2","executionInfo":{"status":"ok","timestamp":1559490989812,"user_tz":420,"elapsed":13198,"user":{"displayName":"Ekaterina Kastrama","photoUrl":"","userId":"16919877199024167270"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["\n","#res_net\n","res_net=ResNet50(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n","print(len(res_net.layers))\n","for layer in res_net.layers[:120]:\n","    layer.trainable = False"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n","  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94658560/94653016 [==============================] - 1s 0us/step\n","175\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2mVk3ox4J8-q","colab_type":"code","outputId":"aa90dbd8-dcb9-467c-b047-f82450e90691","executionInfo":{"status":"ok","timestamp":1559490995624,"user_tz":420,"elapsed":17753,"user":{"displayName":"Ekaterina Kastrama","photoUrl":"","userId":"16919877199024167270"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["#GEM\n","#adapted from https://github.com/jandaldrop/landmark-recognition-challenge/blob/master/landmarks-xception.ipynb\n","\n","X_feat = Input(res_net.output_shape[1:])\n","\n","lambda_layer = Lambda(generalized_mean_pool_2d)  # add input output shape\n","lambda_layer.trainable_weights.extend([gm_exp])\n","X = lambda_layer(X_feat)\n","#X = Dropout(0.05)(X)\n","X = Activation('relu')(X)\n","X = Dense(NUM_CLASSES, activation='softmax')(X)\n","\n","top_model = Model(inputs=X_feat, outputs=X)\n","#top_model.summary()\n","\n","#all together\n","\n","X_image = Input(res_net.input_shape[1:])\n","\n","X_f = res_net(X_image)\n","X_f = top_model(X_f)\n","\n","model = Model(inputs=X_image, outputs=X_f)\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         (None, 128, 128, 3)       0         \n","_________________________________________________________________\n","resnet50 (Model)             (None, 4, 4, 2048)        23587712  \n","_________________________________________________________________\n","model_1 (Model)              (None, 6516)              13351285  \n","=================================================================\n","Total params: 36,938,997\n","Trainable params: 30,566,773\n","Non-trainable params: 6,372,224\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dvF-Kx3dtznZ","colab_type":"code","colab":{}},"source":["from keras.callbacks import History, CSVLogger,ReduceLROnPlateau,ModelCheckpoint\n","history = keras.callbacks.History()\n","\n","\n","model_name='resnet_with_crop_0602'\n","\n","LOG_DIR='/content/drive/My Drive/CS231n/Project/LOG/'\n","Check_DIR='/content/drive/My Drive/CS231n/Project/Checkpoints/'\n","\n","\n","csv_logger = CSVLogger(LOG_DIR+'training_'+model_name+'.log')\n","\n","\n","\n","opt = Adam(0.0001)\n","#opt = Adam(0.00001)\n","\n","checkpoint_name=model_name+'.h5'\n","checkpoint_filepath=Check_DIR+checkpoint_name\n","\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n","                              patience=5, min_lr=0.000001)\n","checkpoint=keras.callbacks.ModelCheckpoint(checkpoint_filepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n","\n","model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[accuracy_class,batch_GAP,binary_crossentropy_n_cat])\n","\n","#check dir\n","if os. path. isfile(checkpoint_filepath):\n","  model.load_weights(checkpoint_filepath)\n","\n","  print(\"Best weights loaded for model \"+ model_name)\n","  \n","model.fit_generator(generator=DataGen(train_urls, batch_size=256+64),\n","                    shuffle=True,\n","                    validation_data=[valid_x, valid_y],\n","                    epochs=40,\n","                    use_multiprocessing=True,\n","                    workers=8,\n","                    verbose=1,\n","                    callbacks=[history,reduce_lr,csv_logger,checkpoint\n","                              ])\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Udpn6JEFAyfw","colab_type":"code","colab":{}},"source":["\n","def val_gap():\n","\n","  result=model.predict(test_x, batch_size=128, verbose=0, steps=None)\n","\n","  true_=test_y.astype('int64')\n","  pred_=result\n","\n","  _,acc=tf.metrics.average_precision_at_k(true_,pred_,1)\n","\n","  custom_gap=batch_GAP(true_,pred_)\n","\n","  sess = tf.Session()\n","  sess.run(tf.local_variables_initializer())\n","\n","  print('Validation GAP: ',sess.run(acc))\n","  print('Custom GAP: ', sess.run(custom_gap))\n","\n","val_gap()\n","\n"],"execution_count":0,"outputs":[]}]}